# BRC-62: Background Evaluation Extended Format (BEEF) Transactions

Deggen (deggen@kschw.com)

## Abstract

We propose a binary format for sending Transactions between peers to allow Simple Payment Verification (SPV). The format is optimized for minimal bandwidth while maintaining data required to independently validate the transaction in full.

Assumption: Every user has an independent source of Block Headers indexed by Merkle Root.

The simplest form is a single transaction, with extended data to allow script evaluation and satoshi amount verification, with Merkle paths for each input.

In cases where one or more inputs are not yet mined, each ancestral transaction is included. We step back through the Transaction DAG until every input has a Merkle Path or corresponding parent transaction.

Inputs with corresponding Merkle Paths will be extended with previous outpoint script and satoshis. Whereas inputs with parents included in stream are not extended since this data has already been sent.**

## Copyright

This BRC is licensed under the Open BSV license.

## Motivation

Simplified Payment Verification formats for transmitting transactions between peers has yet to see wide adoption. This proposal advocates for complete ecosystem adoption of the principles of SPV; acknowledges that the system is secured by economics incentives, and law; and lays out a binary format for transmitting the data between parties optimizing for minimal bandwidth.

Three prior formats should be mentioned:

Extended Format [BRC-30](./0030.md) incorporates the utxo script for script evaluation and satoshis for checking amounts. Ideal for confirmed txs but insufficient for unconfirmed.

[Tx Ancestors](https://tsc.bitcoinsv.com/standards/transaction-ancestors/) which was created for use within the [Lite Client Toolbox](https://docs.bitcoinsv.io), this uses an array of rawtxs, Merkle proofs, and Mapi responses to transport the data required for SPV. 

Everett-style Transaction Envelopes [BRC-8](./0008.md) uses a recursive JSON format which is otherwise similar to the above format originally designed for use within [DPP](../payments/0027.md). One of the ideas which this proposal highlights is that the target of merkle proofs could be height rather than blockhash or root, to save on bytes. Thinking along these lines, we propose that no target be provided at all, and the root simply be calculated at the receieving end as needed. Using the root to look up headers also avoids any implementation complexity associated with competing blocks. For example, when a competing block encapsulates the transactions it may not have the same height, if all transactions are being stored with paths at a height rather than blockhash or merkle root (which are unique to specific versions of a block) then updating a set of the transactions with merkle paths from the new block will be difficult to sort out. If they're indexed by blockhash then it would be trivial to set them all as "in need of updated paths" without the need for disambiguation.

Mapi is to be deprecated in the near future, so any new recommended formats should not include Mapi Responses as part of the solution.

The array of rawtxs within the Tx Ancestors spec makes some sense in that there are strange cases where the same rawtx has two outputs which are spent in different transactions, both within the ancestry of the tx we are validating. You don't want to have embedded a copy of the same proof twice, hence a hash map would make more sense than just including the merkle path bytes in the tx itself. Everett-style Transaction Envelope deals with this well even given its recursive object design. Txids are used as pointers to existing transactions when complex dependency graphs occur. This proposal uses the same thinking, but for a binary format lists are used rather than recursive objects. 

The ordering of data has streaming validation in mind - an idea raised in EF [BRC-30](./0030.md) - such that a receiver can start processing the validation immediately on receipt of the first few bytes, and any later bytes rely on previous bytes for their validation to even begin.

- Merkle Paths
- Oldest Tx Anchored by Path
- Newer Txs depending on Oldest parent
- Newest Tx

As soon as the Merkle Paths are receieved we can calculate the roots and lookup their blockheaders. If they're not valid then validation can stop there - rejected. Then we look at the oldest ancestor - if it's valid then its children can be validated, and so on until we reach the most recent tx.

## Specification

BEEF combines thinking from several formats into one binary stream, prefixed with a header for disambiguation.

- Raw Transaction Format: [BRC-12](./0012.md)
- Extended Format (EF): [BRC-30](./0030.md)
- Compound Merkle Path Format: [BRC-61](./0061.md)

BEEF adds a marker to the transaction format. The BEEF marker allows a library that supports the format to recognize that it is dealing with a transaction in extended format, while a library that does not support extended format will read the transaction as having 0 inputs, 0 outputs and a future nLock time. This has been done to minimize the possible problems a legacy library will have when reading the extended format. It can in no way be recognized as a valid transaction. Lifted from [BRC-30](./0012.md).

| Field                | Description                                                                                            | Size                |
|----------------------|--------------------------------------------------------------------------------------------------------|---------------------|
| Version no           | Used to indicate to nodes which version of BEEF this is.                                               | 4 bytes             |
| BEEF marker          | Marker for Background Evaluation Extended Format **00000000BEEF**                                      | 6 bytes             |
| nPaths               | VarInt number of compound merkle paths to follow                                                       | 1-9 bytes           |
| Compound Merkle Path | All of the paths required to prove inclusion of inputs in longest chain of blocks [BRC-61](./0061.md)  | many bytes          |
| nTransactions        | VarInt number of transactions which follow                                                             | 1-9 bytes           |
| CEF Transaction      | RawTx bytes with input specific extended data as defined below                                         | many bytes          |


### Conditionally Extended Format (CEF) Transaction

This uses the same format detailed in EF [BRC-30](./0012.md) but only for inputs which have been mined and we have included paths for. This means that the data needs to be added to some but not all inputs. Hence we will do away with the global EF flag which is no longer needed since the transaction bytes are coming later in the stream and will not be interpreted as a standard transaction. Instead we will use a single byte to indicate inclusion or not, For sake of clarity when reading the bytes in hex we'll mark "extended format" with `EF` and "not extended" as `00`.

#### CEF Transaction

| Field                     | Description                                                                                   | Size          |
|---------------------------|-----------------------------------------------------------------------------------------------|---------------|
| Version no       | Tx Version                                                                                             | 4 bytes       |
| nIn              | VarInt number of inputs which follow                                                                   | 1 - 9 bytes   |
| **inputs**       | **CEF Input - this is the only difference between RawTX and CEF**                                      | many          |
| nOut             | positive integer VI = [[VarInt]]                                                                       | 1 - 9 bytes   |
| outputs          | Transaction Output Structure                                                                           | many          |
| nLocktime        | if non-zero and sequence numbers are < 0xFFFFFFFF: block height or timestamp when transaction is final | 4 bytes       |

#### CEF Input

In CEF, we extend the input structure to include one of two things. 

1. `00` indicating "no extended data"
2. `EF` indicating "extended format" which includes satoshi amount, previous locking script, and the index of the path associated with the input transaction.

| Field                          | Description                                                                                 | Size             |
|--------------------------------|---------------------------------------------------------------------------------------------|------------------|
| txid                           | Hash of the transaction in which the output was created                                     | 32 bytes         |
| vout                           | Index of the output within that transaction Uint32LE                                        | 4 bytes          |
| unlocking script length        | VarInt Unlocking Script Length                                                              | 1 - 9 bytes      |
| unlocking script               | Unlocking Script                                                                            | many bytes       | 
| Sequence_no                    | For Payment Channels - valid when nSequence = 0xFFFFFFFF                                    | 4 bytes          | 
| **CEF byte**                   | **Either `00` or `EF`**                                                                     | **1 byte**       |
| **Path Index**                 | **VarInt Index of the particular path of the txid within the list of paths**                | **1 - 9 bytes**  |
| **satoshis**                   | **Output value in satoshis of previous input - Uint64LE**                                   | **8 bytes**      |
| **locking script length**      | **VarInt length of locking script**                                                         | **1 - 9 bytes**  |
| **locking script**             | **Script**                                                                                  | **many bytes**   |


### Ordering

Order is important - we must ensure that we end with the tx being evaluated, and its inputs are above, and their inputs are above that. [Khan's algorithm](https://en.wikipedia.org/wiki/Topological_sorting) is a well known solution to the problem in Graph Theory. Running this on the transaction DAG subset is recommended for complex transaction chains where order is not clear. Example below to demonstrate with extraneous data removed for clarity.

```javascript
// khan's algorithm
function khanTopologicalSort(graph) {
    const inDegree = {}
    const queue = []
    const result = []
    for (let node in graph) {
        inDegree[node] = 0
    }
    for (let node in graph) {
        for (let neighbor in graph[node]) {
            inDegree[neighbor]++
        }
    }
    for (let node in inDegree) {
        if (inDegree[node] === 0) {
            queue.push(node)
        }
    }
    while (queue.length) {
        let node = queue.shift()
        result.push(node)
        for (let neighbor in graph[node]) {
            inDegree[neighbor]--
            if (inDegree[neighbor] === 0) {
                queue.push(neighbor)
            }
        }
    }
    return result.reverse()
}

const txs = [
    {
        txid: '2222222222222222222222222222222222222222222222222222222222222222',
        inputs: ['1111111111111111111111111111111111111111111111111111111111111111'],
    },
    {
        txid: '1111111111111111111111111111111111111111111111111111111111111111',
        inputs: ['0000000000000000000000000000000000000000000000000000000000000000'],
    },
    {
        txid: '0000000000000000000000000000000000000000000000000000000000000000',
        inputs: [],
    },
    {
        txid: '4444444444444444444444444444444444444444444444444444444444444444',
        inputs: [
            '3333333333333333333333333333333333333333333333333333333333333333',
            '2222222222222222222222222222222222222222222222222222222222222222',
        ],
    },
    {
        txid: '3333333333333333333333333333333333333333333333333333333333333333',
        inputs: [
            '2222222222222222222222222222222222222222222222222222222222222222',
            '1111111111111111111111111111111111111111111111111111111111111111',
        ],
    },
]

const graph = {}
for (let tx of txs) {
    graph[tx.txid] = {}
    for (let input of tx.inputs) {
        graph[tx.txid][input] = true
    }
}
console.log({ graph })
console.log({ correctOrder: khanTopologicalSort(graph) })
```

## BEEF Example

### Hex
```
0100000000000000BEEF01020101cd73c0c6bb645581816fa960fd2f1636062fcbf23cb57981074ab8d708a76e3b02003470d882cf556a4b943639eba15dc795dffdbebdc98b9a98e3637fda96e3811e01c58e40f22b9e9fcd05a09689a9b19e6e62dbfd3335c5253d09a7a7cd755d9a3c0201da256f78ae0ad74bbf539662cdb9122aa02ba9a9d883f1d52468d96290515adb02b4c8d919190a090e77b73ffcd52b85babaaeeb62da000473102aca7f070facef02020000000158cb8b052fded9a6c450c4212562df8820359ec34da41286421e0d0f2b7eefee000000006a47304402206b1255cb23454c63b22833de25a3a3ecbdb8d8645ad129d3269cdddf10b2ec98022034cadf46e5bfecc38940e5497ddf5fa9aeb37ff5ec3fe8e21b19cbb64a45ec324121029a82bfce319faccc34095c8405896e1223921917501a4f736a04f126d6a01c12ffffffffef00c6c7c903000000001976a9145e506dd7afa889e8f16f5e00555d1c0ab225152f88ac0101000000000000001976a914d866ec5ebb0f4e3840351ee61887101e5407562988ac00000000020000000158cb8b052fded9a6c450c4212562df8820359ec34da41286421e0d0f2b7eefee000000006a47304402206b1255cb23454c63b22833de25a3a3ecbdb8d8645ad129d3269cdddf10b2ec98022034cadf46e5bfecc38940e5497ddf5fa9aeb37ff5ec3fe8e21b19cbb64a45ec324121029a82bfce319faccc34095c8405896e1223921917501a4f736a04f126d6a01c12ffffffff000101000000000000001976a914d866ec5ebb0f4e3840351ee61887101e5407562988ac00000000
```


### Bytewise Breakdown

```javascript
01000000 // version
00000000BEEF
01 // VarInt nPaths = 1
020101cd73c0c6bb645581816fa960fd2f1636062fcbf23cb57981074ab8d708a76e3b02003470d882cf556a4b943639eba15dc795dffdbebdc98b9a98e3637fda96e3811e01c58e40f22b9e9fcd05a09689a9b19e6e62dbfd3335c5253d09a7a7cd755d9a3c0201da256f78ae0ad74bbf539662cdb9122aa02ba9a9d883f1d52468d96290515adb02b4c8d919190a090e77b73ffcd52b85babaaeeb62da000473102aca7f070facef // see BRC-61 for details of compound path format
02 // VarInt nTransactions = 2
//------------------- example tx with a merkle path above -------------------
02000000 // tx version
01 // nInputs
58cb8b052fded9a6c450c4212562df8820359ec34da41286421e0d0f2b7eefee // outpoint txid
00000000 // outpoint vout
6a // length of unlocking script
47304402206b1255cb23454c63b22833de25a3a3ecbdb8d8645ad129d3269cdddf10b2ec98022034cadf46e5bfecc38940e5497ddf5fa9aeb37ff5ec3fe8e21b19cbb64a45ec324121029a82bfce319faccc34095c8405896e1223921917501a4f736a04f126d6a01c12 // unlocking script
ffffffff // nSequence
// CONDITIONAL EXTENDED DATA
    ef // indicates that extended data follows
    00 // VarInt index of associated path above
    c6c7c90300000000 // satoshis
    19 // lenLockingScript
    76a9145e506dd7afa889e8f16f5e00555d1c0ab225152f88ac // lockingScript
// END OF INPUT
01 // nOutputs
0100000000000000 // satoshis
19 // locking script length
76a914d866ec5ebb0f4e3840351ee61887101e5407562988ac // locking script
00000000 //  nLocktime
//------------------- example tx with parent above -------------------
02000000 // tx version
01 // nInputs
58cb8b052fded9a6c450c4212562df8820359ec34da41286421e0d0f2b7eefee // outpoint txid
00000000 // outpoint vout
6a // length of unlocking script
47304402206b1255cb23454c63b22833de25a3a3ecbdb8d8645ad129d3269cdddf10b2ec98022034cadf46e5bfecc38940e5497ddf5fa9aeb37ff5ec3fe8e21b19cbb64a45ec324121029a82bfce319faccc34095c8405896e1223921917501a4f736a04f126d6a01c12 // unlocking script
ffffffff // nSequence
// CONDITIONAL EXTENDED DATA
    00 // indicates that no extended data follows
// END OF INPUT
01 // nOutputs
0100000000000000 // satoshis
19 // locking script length
76a914d866ec5ebb0f4e3840351ee61887101e5407562988ac // locking script
00000000 //  nLocktime
```

## Backward compatibility

The Background Evaluation Extended Format is not backwards compatible, but has been designed in such a way that existing software should not read a transaction in Background Evaluation Extended Format as a valid (partial) transaction. The Background Evaluation Extended Format header (00000000BEEF) will be read as an empty transaction with a future nLock time in a library that does not support the Background Evaluation Extended Format. 

This approach is a direct lift from EF [BIP-30](./0030.md).

## Implementation

The format should be built into common libraries such as [bsv](https://www.npmjs.com/package/bsv) and [go-bt](https://github.com/libsv/go-bt) for easy incorporation into existing stacks.  
